{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Disclaimer: We use some advanced packages here without detailed explanation. You can use these, but we do not provide any support.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To install them, you can uncomment the following lines:\n",
    "# (%pip will call pip from the currently active python environment)\n",
    "# %pip install scikit-learn\n",
    "\n",
    "# Note: Some of these packages are still not compatible with Python 3.12 yet\n",
    "# %pip install sweetviz\n",
    "# %pip install ydata_profiling\n",
    "# %pip install shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRISP-DM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Note: The following do not work with Python 3.12\n",
    "#import shap\n",
    "#from ydata_profiling import ProfileReport\n",
    "#import sweetviz as sv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reproducibility \n",
    "\n",
    "A best practice in data analytics projects is to work with *seeds* to ensure the reproducability of results. \n",
    "This is especially important in the Analytics Cup, since the rules require you to write a self-contained\n",
    "script that produces reproducable results. \n",
    "\n",
    "To achieve this, we can set seeds for all used random number generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 55"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 1: Business Understanding\n",
    "\n",
    "Serves to assess use cases, feasibility, requirements, and\n",
    "risks of the endeavored data driven project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Startup that suggests new recipes to users\\\n",
    "But we have been having many cancelations of subscriptions\\\n",
    "Problem was that the users found that the recipes suggested (even though they had high quality) did not match the customer's diet and needs\\\n",
    "Now we have a system of likes and dislikes for the recipes and a new user interface, where the users can enter information about what they want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 2: Data Understanding\n",
    "\n",
    "Assess the data quality and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "diet = pd.read_csv(\"diet.csv\")\n",
    "recipes = pd.read_csv(\"recipes.csv\")\n",
    "requests = pd.read_csv(\"requests.csv\")\n",
    "reviews = pd.read_csv(\"reviews.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "have a look at the data and its attributes \\\n",
    "check if columns are properly named \\\n",
    "general overview over data, check for missing values, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diet pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diet[\"Diet\"] = diet[\"Diet\"].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recipes pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change types of column\n",
    "def refactorIngredients(ingredients):\n",
    "    if ingredients == \"character(0)\":\n",
    "        return []\n",
    "    ingredients = ingredients.replace(\"\\\\\", '').replace(\"\\\"\", '').replace('c(','').replace(')', '')\n",
    "    ingredients = ingredients.split(\",\")\n",
    "    return ingredients\n",
    "\n",
    "recipes[\"RecipeIngredientQuantities\"] = recipes[\"RecipeIngredientQuantities\"].apply(lambda x: refactorIngredients(x))\n",
    "recipes[\"RecipeIngredientParts\"] = recipes[\"RecipeIngredientParts\"].apply(lambda x: refactorIngredients(x))\n",
    "\n",
    "recipes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determines if recipe is veggie, vegan or omnivore\n",
    "def categorizeRecipe(ingredients):\n",
    "    meat_derivates = [\"pork\", \"beef\", \"meat\", \"fish\", \"tuna\", \"chicken\", \"squid\", \"schrimp\", \"trout\", \"mussels\", \n",
    "                      \"fillet\", \"lamb\", \"scallops\", \"sardine\", \"salmon\", \"lobster\", \"steak\", \"bacon\", \"ham\", \"oyster\"]\n",
    "    animal_derivates = [\"milk\", \"egg\", \"honey\", \"gelatin\", \"butter\", \"mayonnaise\", \"cheese\", \"margarine\", \n",
    "                    \" heavy\", \"yogurt\", \"pudding\", \"shortening\", \"ice cream\", \"chocolate\", \"alfredo\", \"Miracle Whip\", \"half-and-half\"]\n",
    "    vegan_exclusions = [\"substitute\", \"peanut\", \"apple\", \"vegan\", \"soymilk\"]\n",
    "    vegan = True\n",
    "    for ingredient in ingredients:\n",
    "        if any(word in ingredient.lower() for word in meat_derivates):\n",
    "            return \"Omnivore\"\n",
    "        if ingredient in vegan_exclusions:\n",
    "            continue\n",
    "        if any(word in ingredient.lower() for word in animal_derivates):\n",
    "            vegan = False\n",
    "    if vegan: \n",
    "        return \"Vegan\"\n",
    "    else: \n",
    "        return \"Vegetarian\"\n",
    "\n",
    "recipes[\"RecipeDiet\"] = recipes[\"RecipeIngredientParts\"].apply(lambda x: categorizeRecipe(x))\n",
    "recipes['RecipeDiet'] = recipes['RecipeDiet'].astype('category')\n",
    "\n",
    "# Create another table \"recipe extra info\" columns category, ingredient quatities, parts\n",
    "selected_columns = ['RecipeCategory', 'RecipeIngredientQuantities', 'RecipeIngredientParts', 'RecipeServings', 'RecipeYield']\n",
    "recipe_extra_info = recipes[selected_columns]\n",
    "recipes = recipes.drop(columns=selected_columns)\n",
    "\n",
    "recipes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requests pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.info()\n",
    "# no missing values: GOOD!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming the columns\n",
    "requests = requests.rename(columns={\"HighCalories\": \"Calories\", \"HighProtein\":\"Protein\", \"LowFat\": \"Fat\", \"LowSugar\": \"Sugar\", \"HighFiber\":\"Fiber\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardizing column Calorie to the same format\n",
    "requests[\"Calories\"] = requests[\"Calories\"].astype(\"int\")\n",
    "\n",
    "# standardizing column Protein Yes->1\n",
    "requests[\"Protein\"] = requests[\"Protein\"].replace(\"Yes\",\"1\")\n",
    "\n",
    "# changing 0 -> 1 in column Sugar \n",
    "requests[\"Sugar\"] = requests[\"Sugar\"].replace(\"0\",\"1\")\n",
    "\n",
    "# changing 0 -> 1 and 1 -> 0  column Fat\n",
    "#requests[\"Fat\"] = requests[\"Fat\"].replace({1 : 0, 0 : 1})\n",
    "requests[\"Fat\"] = 1 - requests[\"Fat\"]\n",
    "\n",
    "# transforming macronutrients columns -> categories \n",
    "requests[[\"Calories\", \"Protein\", \"Fiber\", \"Sugar\", \"Fat\"]] = requests[[\"Calories\", \"Protein\", \"Fiber\", \"Sugar\", \"Fat\"]].astype(\"category\")\n",
    "\n",
    "requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reviews pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews.drop(columns = [\"Rating\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "df_grouped_by_class = df.groupby(by=\"variety\")\n",
    "\n",
    "df_setosa = df_grouped_by_class.get_group(\"Setosa\")\n",
    "df_versicolor = df_grouped_by_class.get_group(\"Versicolor\")\n",
    "df_virginica = df_grouped_by_class.get_group(\"Virginica\")\n",
    "\n",
    "class_labels = {\n",
    "    \"Setosa\" : {\n",
    "        \"color\" : \"blue\",\n",
    "        \"data\" : df_setosa\n",
    "    },\n",
    "    \"Versicolor\" : {\n",
    "        \"color\" : \"green\",\n",
    "        \"data\" : df_versicolor\n",
    "    },\n",
    "    \"Virginica\" : {\n",
    "        \"color\" : \"red\",\n",
    "        \"data\" : df_virginica\n",
    "    }\n",
    "}\n",
    "\n",
    "for class_i in class_labels:\n",
    "    class_color = class_labels[class_i][\"color\"]\n",
    "    class_df = class_labels[class_i][\"data\"]\n",
    "    p = sns.pairplot(class_df, diag_kind=\"hist\", diag_kws={\"color\" : class_color}, plot_kws={\"color\" : class_color, \"label\" : class_i})\n",
    "    p.fig.suptitle(class_i, y=1.0, size=15)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# We can also leverage the dataprep package to get a nice summary report\n",
    "report = sv.analyze(df)\n",
    "report.show_notebook()\n",
    "\n",
    "# We can also leverage the yadata_profiling package to get a nice summary report\n",
    "profile = ProfileReport(df, title=\"Iris Data - Summary Report\")\n",
    "profile\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 3: Data Preparation\n",
    "\n",
    "The goal is assure data quality: includes removing wrong/corrupt \n",
    "data entries and making sure the entries are standardized, e.g. enforcing certain encodings. \n",
    "Then transforms the data in order to make it suitable for the modelling step. This includes scaling, dimensionality\n",
    "reduction, data augmentation, outlier removal, etc.\\\n",
    " \\\n",
    "In practise, this will rarely be the case. On average, this step takes up to **80%** of \n",
    "the time of the whole project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To do: transform categorical feature into categorical variables (exemplo df[\"variety\"] = df[\"variety\"].astype(\"category\"))\n",
    "# fill/remove/change missing/corrupt values\n",
    "\n",
    "# To do: ver se precisamos standardize alguma feature (exemplo na celula seguinte com o StandardScaler), se precisamos imputar valores em registros com valores nulos, \n",
    "# se precisamos lidar com outliers, se precisamos usar alguma estretégia de redução de dimensionalidade (tipo PCA na próxima celula)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# data scaling\n",
    "transform_scaler = StandardScaler()\n",
    "\n",
    "# dimensionality reduction\n",
    "transform_pca = PCA()\n",
    "\n",
    "# value imputing\n",
    "\n",
    "# outlier detection/removal\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join das 4 tabelas\n",
    "- há users na tabela \"diet\" que nao estao na tabela \"reviews\" -- Ok!\n",
    "- match perfeito de recipeid and authorid entre requests e reviews -- Otimo!\n",
    "- todas as receitas de \"recipes\" estao sendo mostradas para pelo menos um usuario -- Ok!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tabelas: diet, requests, reviews, recipes\n",
    "dietrequestsmerged = diet.merge(requests, on = [\"AuthorId\"])\n",
    "dietrequestsreviewsmerged = dietrequestsmerged.merge(reviews, on = [\"AuthorId\", \"RecipeId\"])\n",
    "dietrequestsreviewsmerged = dietrequestsreviewsmerged.rename(columns={\"Calories\" : \"Requested_Calories\"})\n",
    "mergedtables = dietrequestsreviewsmerged.merge(recipes, on = [\"RecipeId\"])\n",
    "mergedtables = mergedtables.rename(columns={\"Calories\" : \"Recipe_Calories\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissiondataset = mergedtables[mergedtables[\"Like\"].isna()] #com Null na coluna Like\n",
    "trainandtestdataset = mergedtables[mergedtables[\"Like\"].notna()] #sem Null na coluna Like\n",
    "\n",
    "trainandtestdataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split our data set into *train* and *test* data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: ver se vamos usar um split para validação, ou usar cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Drop columns that should not be considered\n",
    "# Drop Name because is string and Random Forest doesn't accept strings\n",
    "trainandtestdataset = trainandtestdataset.drop(columns=['AuthorId', 'RecipeId', 'TestSetId', 'Name'])\n",
    "\n",
    "# Drop categorical values and transform them into one column for each of possible categories\n",
    "# This also removes remaining string values\n",
    "# ATTENTION: Eu nao sei se essa parte eh necessaria para o Linear Regression. Acredito que sim, mas, se nao, reorganizamos o codigo de repente\n",
    "categorical_values = ['Diet', 'RecipeDiet', 'Requested_Calories', 'Protein', 'Fat', 'Sugar', 'Fiber']\n",
    "\n",
    "for column in categorical_values:\n",
    "    new_data = pd.get_dummies(trainandtestdataset[column], prefix=column)\n",
    "    trainandtestdataset = pd.concat([trainandtestdataset, new_data], axis=1)\n",
    "    \n",
    "trainandtestdataset = trainandtestdataset.drop(columns=categorical_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate train and test data and X and Y variables\n",
    "\n",
    "X_features = trainandtestdataset.drop(columns=\"Like\")\n",
    "Y_classes = trainandtestdataset[\"Like\"]\n",
    "Y_classes = Y_classes.astype('category')\n",
    "\n",
    "trainandtestdataset.info()\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_features, Y_classes,\n",
    "                                                    test_size=0.2, \n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=seed) # for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- X_train: 77.904 rows × 24 columns\n",
    "- Y_train: 77.904 rows\n",
    "- X_test: 19.477 rows × 24 columns\n",
    "- Y_test: 19.477 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 4: Modeling\n",
    "\n",
    "In this phase, the model is trained and tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_forest = RandomForestClassifier()\n",
    "\n",
    "random_forest.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = random_forest.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "confusion_matrix = confusion_matrix(Y_test, Y_pred)\n",
    "print(confusion_matrix)\n",
    "\n",
    "true_negatives = confusion_matrix[0][0]\n",
    "false_negatives = confusion_matrix[1][0]\n",
    "false_positives = confusion_matrix[0][1]\n",
    "true_positives = confusion_matrix[1][1]\n",
    "\n",
    "sensitivity = true_positives / (true_positives + false_negatives)\n",
    "specificity = true_negatives / (true_negatives + false_positives)\n",
    "\n",
    "print(\"sensitivity = \", sensitivity)\n",
    "print(\"specificity = \", specificity)\n",
    "\n",
    "# Too many False predictions\n",
    "\n",
    "# Possible ways to improve\n",
    "# Re add the recipe name in some way - parse the string and see if the title is vegetarian. \n",
    "# Group the cook time in discrete chunks?\n",
    "# Group the other nutritional facts columns of recipe in discrete chunks?\n",
    "# Group age in chunks ?\n",
    "# Drop some columns from recipe like sodium \n",
    "# Reduce dimensionality. I guess fat, saturated fat and cholesterol are correlated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, you want to find the best classifier. As candidates, consider\n",
    "#   1. LogisticRegression\n",
    "#   2. RandomForestClassifier\n",
    "#   3. other algorithms from sklearn (easy to add)\n",
    "#   4. custom algorithms (more difficult to implement)\n",
    "    \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model_logistic_regression = LogisticRegression(max_iter=30)\n",
    "model_random_forest = RandomForestClassifier()\n",
    "model_gradient_boosting = GradientBoostingClassifier()\n",
    "\n",
    "# train the models\n",
    "pipeline = Pipeline(steps=[(\"scaler\", transform_scaler), \n",
    "                           (\"pca\", transform_pca),\n",
    "                           (\"model\", None)])\n",
    "\n",
    "parameter_grid_preprocessing = {\n",
    "  \"pca__n_components\" : [1, 2, 3, 4],\n",
    "}\n",
    "\n",
    "parameter_grid_logistic_regression = {\n",
    "  \"model\" : [model_logistic_regression],\n",
    "  \"model__C\" : [0.1, 1, 10],  # inverse regularization strength\n",
    "}\n",
    "\n",
    "parameter_grid_gradient_boosting = {\n",
    "  \"model\" : [model_gradient_boosting],\n",
    "  \"model__n_estimators\" : [10, 20, 30]\n",
    "}\n",
    "\n",
    "parameter_grid_random_forest = {\n",
    "  \"model\" : [model_random_forest],\n",
    "  \"model__n_estimators\" : [10, 20, 50],  # number of max trees in the forest\n",
    "  \"model__max_depth\" : [2, 3, 4],\n",
    "}\n",
    "\n",
    "meta_parameter_grid = [parameter_grid_logistic_regression,\n",
    "                       parameter_grid_random_forest,\n",
    "                       parameter_grid_gradient_boosting]\n",
    "\n",
    "meta_parameter_grid = [{**parameter_grid_preprocessing, **model_grid}\n",
    "                       for model_grid in meta_parameter_grid]\n",
    "\n",
    "search = GridSearchCV(pipeline,\n",
    "                      meta_parameter_grid, \n",
    "                      scoring=\"balanced_accuracy\",\n",
    "                      n_jobs=2, \n",
    "                      cv=5,  # number of folds for cross-validation \n",
    "                      error_score=\"raise\"\n",
    ")\n",
    "\n",
    "# here, the actual training and grid search happens\n",
    "search.fit(X_train, Y_train.values.ravel())\n",
    "\n",
    "print(\"best parameter:\", search.best_params_ ,\"(CV score=%0.3f)\" % search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Evaluation\n",
    "\n",
    "Once the appropriate models are chosen, they are evaluated on the test set. For\n",
    "this, different evaluation metrics can be used. Furthermore, this step is where\n",
    "the models and their predictions are analyzed resp. different properties, including\n",
    "feature importance, robustness to outliers, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate performance of model on test set\n",
    "print(\"Score on test set:\", search.score(X_test, Y_test.values.ravel()))\n",
    "\n",
    "# contingency table\n",
    "ct = pd.crosstab(search.best_estimator_.predict(X_test), Y_test.values.ravel(),\n",
    "                 rownames=[\"pred\"], colnames=[\"true\"])\n",
    "print(ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (optional, if you're curious) \n",
    "# for a detailed look on the performance of the different models\n",
    "def get_search_score_overview():\n",
    "  for c,s in zip(search.cv_results_[\"params\"],search.cv_results_[\"mean_test_score\"]):\n",
    "      print(c, s)\n",
    "\n",
    "print(get_search_score_overview())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretability\n",
    "\n",
    "##### Disclaimer: This only works if shap is installed.\n",
    "\n",
    "In addition to models and their predictions, it is often important to understand _why_ a model makes certain predictions. \n",
    "There is a lot of literature on how this can be achieved (explainability), but we will only show the use of Shapley values\n",
    "using the python module \"shap\", which is a combination of Shapley values and LIME. \n",
    "You can find more information on this topic [here](https://christophm.github.io/interpretable-ml-book/shap.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume random forest model\n",
    "model = RandomForestClassifier(n_estimators=10, random_state=seed)\n",
    "model.fit(X_train, Y_train.values.ravel())\n",
    "\n",
    "# compute shapley values\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_train)\n",
    "shap_interaction_values = explainer.shap_interaction_values(X_train)\n",
    "\n",
    "expected_value = explainer.expected_value\n",
    "print(expected_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class dependent plots of shapley values for each feature\n",
    "for i,c in enumerate(df.variety.unique()):\n",
    "    shap.summary_plot(shap_values[i], X_train, show=False)\n",
    "    plt.title(\"Shapley values for \"+str(c))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the computed SHAP values, we can interpret that the *petal.width* has a positive impact on the output of the model \n",
    "if the feature value is moderate. For high aand low values, the impact is negative. The same observation\n",
    "holds for *petal.length*. Besides, the impact of the *sepal.length* and *sepal.width* features are rather low. By impact on a \n",
    "the target, we model the probability that we classify that target. Thus, if *petal.width* is high, it is more likely\n",
    "that we classify the data point as Versicolor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Deployment\n",
    "\n",
    "Now that you have chosen and trained your model, it is time to deploy it to your\n",
    "clients system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def micro_service_classify_iris(datapoint):\n",
    "    \n",
    "  # make sure the provided datapoints adhere to the correct format for model input\n",
    "\n",
    "  # fetch your trained model\n",
    "  model = search.best_estimator_\n",
    "\n",
    "  # make prediction with the model\n",
    "  prediction = model.predict(datapoint)\n",
    "\n",
    "  return prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Analytics Cup, you need to export your prediction in a very specific output format. This is a csv file without an index and two columns, *id* and *prediction*. Note that the values in both columns need to be integer values, and especially in the *prediction* column either 1 or 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do: arrumar a celula abaixo com os nossos dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's assume that our id column is the index of the dataframe\n",
    "output = pd.DataFrame(df_flowers.variety)\n",
    "output['id'] = df_flowers.index\n",
    "output = output.rename(columns={'variety': 'prediction'})\n",
    "output = output.reindex(columns=[\"id\", \"prediction\"])\n",
    "output.to_csv('analzticscuppredictionfile.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
