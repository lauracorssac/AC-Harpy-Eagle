{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRISP-DM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reproducibility \n",
    "\n",
    "A best practice in data analytics projects is to work with *seeds* to ensure the reproducability of results. \n",
    "This is especially important in the Analytics Cup, since the rules require you to write a self-contained\n",
    "script that produces reproducable results. \n",
    "\n",
    "To achieve this, we can set seeds for all used random number generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 55"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 1: Business Understanding\n",
    "\n",
    "Serves to assess use cases, feasibility, requirements, and\n",
    "risks of the endeavored data driven project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Startup that suggests new recipes to users\\\n",
    "But we have been having many cancelations of subscriptions\\\n",
    "Problem was that the users found that the recipes suggested (even though they had high quality) did not match the customer's diet and needs\\\n",
    "Now we have a system of likes and dislikes for the recipes and a new user interface, where the users can enter information about what they want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 2: Data Understanding\n",
    "\n",
    "Assess the data quality and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "diet = pd.read_csv(\"diet.csv\")\n",
    "recipes = pd.read_csv(\"recipes.csv\")\n",
    "requests = pd.read_csv(\"requests.csv\")\n",
    "reviews = pd.read_csv(\"reviews.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "have a look at the data and its attributes \\\n",
    "check if columns are properly named \\\n",
    "general overview over data, check for missing values, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diet pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diet[\"Diet\"] = diet[\"Diet\"].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recipes pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change types of column\n",
    "def refactorIngredients(ingredients):\n",
    "    if ingredients == \"character(0)\":\n",
    "        return []\n",
    "    ingredients = ingredients.replace(\"\\\\\", '').replace(\"\\\"\", '').replace('c(','').replace(')', '')\n",
    "    ingredients = ingredients.split(\",\")\n",
    "    return ingredients\n",
    "\n",
    "recipes[\"RecipeIngredientQuantities\"] = recipes[\"RecipeIngredientQuantities\"].apply(lambda x: refactorIngredients(x))\n",
    "recipes[\"RecipeIngredientParts\"] = recipes[\"RecipeIngredientParts\"].apply(lambda x: refactorIngredients(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determines if recipe is veggie, vegan or omnivore\n",
    "def categorizeRecipe(ingredients):\n",
    "    meat_derivates = [\"pork\", \"beef\", \"meat\", \"fish\", \"tuna\", \"chicken\", \"squid\", \"schrimp\", \"trout\", \"mussels\", \n",
    "                      \"fillet\", \"lamb\", \"scallops\", \"sardine\", \"salmon\", \"lobster\", \"steak\", \"bacon\", \"ham\", \"oyster\"]\n",
    "    animal_derivates = [\"milk\", \"egg\", \"honey\", \"gelatin\", \"butter\", \"mayonnaise\", \"cheese\", \"margarine\", \n",
    "                    \" heavy\", \"yogurt\", \"pudding\", \"shortening\", \"ice cream\", \"chocolate\", \"alfredo\", \"Miracle Whip\", \"half-and-half\"]\n",
    "    vegan_exclusions = [\"substitute\", \"peanut\", \"apple\", \"vegan\", \"soymilk\"]\n",
    "    vegan = True\n",
    "    for ingredient in ingredients:\n",
    "        if any(word in ingredient.lower() for word in meat_derivates):\n",
    "            return \"Omnivore\"\n",
    "        if ingredient in vegan_exclusions:\n",
    "            continue\n",
    "        if any(word in ingredient.lower() for word in animal_derivates):\n",
    "            vegan = False\n",
    "    if vegan: \n",
    "        return \"Vegan\"\n",
    "    else: \n",
    "        return \"Vegetarian\"\n",
    "\n",
    "recipes[\"RecipeDiet\"] = recipes[\"RecipeIngredientParts\"].apply(lambda x: categorizeRecipe(x))\n",
    "recipes['RecipeDiet'] = recipes['RecipeDiet'].astype('category')\n",
    "\n",
    "# Create another table \"recipe extra info\" columns category, ingredient quatities, parts\n",
    "selected_columns = ['RecipeCategory', 'RecipeIngredientQuantities', 'RecipeIngredientParts', 'RecipeServings', 'RecipeYield']\n",
    "recipe_extra_info = recipes[selected_columns]\n",
    "recipes = recipes.drop(columns=selected_columns)\n",
    "\n",
    "recipes\n",
    "\n",
    "recipe_extra_info.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requests pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming the columns\n",
    "requests = requests.rename(columns={\"HighCalories\": \"Calories\", \"HighProtein\":\"Protein\", \"LowFat\": \"Fat\", \"LowSugar\": \"Sugar\", \"HighFiber\":\"Fiber\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardizing column Calorie to the same format\n",
    "requests[\"Calories\"] = requests[\"Calories\"].astype(\"int\")\n",
    "\n",
    "# standardizing column Protein Yes->1\n",
    "requests[\"Protein\"] = requests[\"Protein\"].replace(\"Yes\",\"1\")\n",
    "\n",
    "# changing 0 -> 1 in column Sugar \n",
    "requests[\"Sugar\"] = requests[\"Sugar\"].replace(\"0\",\"1\")\n",
    "\n",
    "# changing 0 -> 1 and 1 -> 0  column Fat\n",
    "#requests[\"Fat\"] = requests[\"Fat\"].replace({1 : 0, 0 : 1})\n",
    "requests[\"Fat\"] = 1 - requests[\"Fat\"]\n",
    "\n",
    "# transforming macronutrients columns -> categories \n",
    "#requests[[\"Calories\", \"Protein\", \"Fiber\", \"Sugar\",\"]] = requests[[\"Calories\", \"Protein\", \"Fiber\", \"Sugar\", \"Fat\"]].astype(\"category\")\n",
    "\n",
    "requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reviews pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews.drop(columns = [\"Rating\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 3: Data Preparation\n",
    "\n",
    "The goal is assure data quality: includes removing wrong/corrupt \n",
    "data entries and making sure the entries are standardized, e.g. enforcing certain encodings. \n",
    "Then transforms the data in order to make it suitable for the modelling step. This includes scaling, dimensionality\n",
    "reduction, data augmentation, outlier removal, etc.\\\n",
    " \\\n",
    "In practise, this will rarely be the case. On average, this step takes up to **80%** of \n",
    "the time of the whole project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tabelas: diet, requests, reviews, recipes\n",
    "dietrequestsmerged = diet.merge(requests, on = [\"AuthorId\"])\n",
    "dietrequestsreviewsmerged = dietrequestsmerged.merge(reviews, on = [\"AuthorId\", \"RecipeId\"])\n",
    "dietrequestsreviewsmerged = dietrequestsreviewsmerged.rename(columns={\"Calories\" : \"Requested_Calories\"})\n",
    "mergedtables = dietrequestsreviewsmerged.merge(recipes, on = [\"RecipeId\"])\n",
    "mergedtables = mergedtables.rename(columns={\"Calories\" : \"Recipe_Calories\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedtables[\"Total_time_recipe\"] = mergedtables[\"CookTime\"] + mergedtables[\"PrepTime\"]\n",
    "mergedtables = mergedtables.drop(columns=[\"PrepTime\", \"CookTime\"])\n",
    "mergedtables[\"Time\"] = np.where(mergedtables[\"Time\"] < 0, 28_000_000, mergedtables[\"Time\"])\n",
    "mergedtables[\"Recipe_Time_Match\"] = (mergedtables[\"Total_time_recipe\"] <= (1.2 * mergedtables[\"Time\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedtables[(mergedtables[\"Recipe_Time_Match\"] == False)][[\"Recipe_Time_Match\", \"Total_time_recipe\", \"Time\"]]\n",
    "mergedtables = mergedtables.drop(columns=[\"Time\", \"Total_time_recipe\"])\n",
    "mergedtables = mergedtables.drop(columns=[\"SaturatedFatContent\", \"CholesterolContent\", \"SodiumContent\", \"CarbohydrateContent\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diet_match(person_diet, recipe_diet):\n",
    "    if person_diet == \"Omnivore\":\n",
    "        return True\n",
    "    if person_diet == \"Vegetarian\" and recipe_diet != \"Omnivore\":\n",
    "        return True\n",
    "    if person_diet == \"Vegan\" and recipe_diet == \"Vegan\":\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "mergedtables[\"RecipeMatch\"] = mergedtables.apply(lambda row: diet_match(row[\"Diet\"], row[\"RecipeDiet\"]), axis= 1)\n",
    "\n",
    "mergedtables[[\"RecipeMatch\", \"Diet\", \"RecipeDiet\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedtables = mergedtables.drop(columns=[\"Diet\", \"RecipeDiet\"])\n",
    "\n",
    "categorical_values = [\"Requested_Calories\", \"Protein\", \"Fat\", \"Sugar\", \"Fiber\"] # 'Diet', 'RecipeDiet', \n",
    "\n",
    "# Drop categorical values and transform them into one column for each of possible categories\n",
    "# This also removes remaining string values\n",
    "for column in categorical_values:\n",
    "    new_data = pd.get_dummies(mergedtables[column], prefix=column)\n",
    "    mergedtables = pd.concat([mergedtables, new_data], axis=1)\n",
    "\n",
    "mergedtables = mergedtables.drop(columns=categorical_values)\n",
    "\n",
    "# Drop columns that should not be considered\n",
    "# Drop Name because is string and Random Forest doesn't accept strings\n",
    "\n",
    "submissiondataset = mergedtables[mergedtables[\"Like\"].isna()] #com Null na coluna Like\n",
    "trainandtestdataset = mergedtables[mergedtables[\"Like\"].notna()] #sem Null na coluna Like\n",
    "\n",
    "selected_columns_test = ['AuthorId', 'RecipeId', 'TestSetId', 'Name', \"Sugar_1\", \"Fat_0\"]\n",
    "submission_extra_info = submissiondataset[selected_columns_test]\n",
    "training_extra_ingo = trainandtestdataset[selected_columns_test]\n",
    "\n",
    "submissiondataset = submissiondataset.drop(columns= selected_columns_test)\n",
    "trainandtestdataset = trainandtestdataset.drop(columns= selected_columns_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split our data set into *train* and *test* data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Under + Oversampling\n",
    "\n",
    "df_majority = trainandtestdataset[trainandtestdataset['Like'] == 0]\n",
    "df_minority = trainandtestdataset[trainandtestdataset['Like'] == 1]\n",
    "\n",
    "length_majority = len(df_majority)\n",
    "length_minority = len(df_minority)\n",
    "difference = length_majority - length_minority\n",
    "final_length = int(difference * 0.7 + length_minority)\n",
    "\n",
    "print(length_majority, length_minority)\n",
    "# Downsample the majority class\n",
    "df_majority = resample(df_majority, replace=False, n_samples= final_length, random_state=seed)\n",
    "\n",
    "# Oversample the minority class\n",
    "df_minority = resample(df_minority, replace=True, n_samples= final_length, random_state=seed)\n",
    "\n",
    "# Combine the downsampled majority class with the original minority class\n",
    "trainandtestdataset = pd.concat([df_minority, df_majority])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate train and test data and X and Y variables\n",
    "\n",
    "X_features = trainandtestdataset.drop(columns=\"Like\")\n",
    "Y_classes = trainandtestdataset[\"Like\"]\n",
    "Y_classes = Y_classes.astype('category')\n",
    "\n",
    "trainandtestdataset.info()\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_features, Y_classes,\n",
    "                                                    test_size=0.2, \n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=seed) # for reproducibility\n",
    "train_df = X_train\n",
    "train_df[\"Y_train\"] = Y_train\n",
    "\n",
    "columns_with_outliers = [\"Recipe_Calories\", \"FatContent\", \"SugarContent\", \"ProteinContent\", \"FiberContent\"]\n",
    "\n",
    "for item in columns_with_outliers:\n",
    "    mean = np.mean(train_df[item])\n",
    "    std = train_df[item].std()\n",
    "    upper_bound = (mean + 5 * std)\n",
    "    lower_bound = max(0, (mean - 5 * std))\n",
    "    train_df = train_df[(train_df[item] >= lower_bound) & (train_df[item] < upper_bound)]\n",
    "\n",
    "train_df = train_df.drop(columns=[\"Recipe_Calories\"])\n",
    "X_test = X_test.drop(columns=[\"Recipe_Calories\"])\n",
    "submissiondataset = submissiondataset.drop(columns=[\"Recipe_Calories\"])\n",
    "\n",
    "X_train = train_df.drop(columns=[\"Y_train\"])\n",
    "Y_train = train_df[\"Y_train\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- X_train: 77.904 rows × 24 columns\n",
    "- Y_train: 77.904 rows\n",
    "- X_test: 19.477 rows × 24 columns\n",
    "- Y_test: 19.477 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 4: Modeling\n",
    "\n",
    "In this phase, the model is trained and tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_forest = RandomForestClassifier()\n",
    "\n",
    "random_forest.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.columns)\n",
    "Y_pred = random_forest.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "confusion_matrix = confusion_matrix(Y_test, Y_pred)\n",
    "print(confusion_matrix)\n",
    "\n",
    "true_negatives = confusion_matrix[0][0]\n",
    "false_negatives = confusion_matrix[1][0]\n",
    "false_positives = confusion_matrix[0][1]\n",
    "true_positives = confusion_matrix[1][1]\n",
    "\n",
    "sensitivity = true_positives / (true_positives + false_negatives)\n",
    "specificity = true_negatives / (true_negatives + false_positives)\n",
    "\n",
    "print(\"sensitivity = \", sensitivity)\n",
    "print(\"specificity = \", specificity)\n",
    "print(Y_test, Y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(specificity + sensitivity) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features_submission = submissiondataset.drop(columns=\"Like\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submissiion\n",
    "\n",
    "# Let's assume that our id column is the index of the dataframe\n",
    "\n",
    "id = submission_extra_info['TestSetId']\n",
    "Y_pred_submission = random_forest.predict(X_features_submission)\n",
    "\n",
    "\n",
    "output = pd.DataFrame({'id': id, 'prediction': Y_pred_submission})\n",
    "\n",
    "#output\n",
    "output.info()\n",
    "output['id'] = output[\"id\"].astype('int')\n",
    "output['prediction'] = output[\"prediction\"].astype('int')\n",
    "output.to_csv('analzticscuppredictionfile.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(Y_pred_submission), len(id))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
