{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Disclaimer: We use some advanced packages here without detailed explanation. You can use these, but we do not provide any support.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To install them, you can uncomment the following lines:\n",
    "# (%pip will call pip from the currently active python environment)\n",
    "# %pip install scikit-learn\n",
    "\n",
    "# Note: Some of these packages are still not compatible with Python 3.12 yet\n",
    "# %pip install sweetviz\n",
    "# %pip install ydata_profiling\n",
    "# %pip install shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRISP-DM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Note: The following do not work with Python 3.12\n",
    "#import shap\n",
    "#from ydata_profiling import ProfileReport\n",
    "#import sweetviz as sv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reproducibility \n",
    "\n",
    "A best practice in data analytics projects is to work with *seeds* to ensure the reproducability of results. \n",
    "This is especially important in the Analytics Cup, since the rules require you to write a self-contained\n",
    "script that produces reproducable results. \n",
    "\n",
    "To achieve this, we can set seeds for all used random number generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 55"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 1: Business Understanding\n",
    "\n",
    "Serves to assess use cases, feasibility, requirements, and\n",
    "risks of the endeavored data driven project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Startup that suggests new recipes to users\\\n",
    "But we have been having many cancelations of subscriptions\\\n",
    "Problem was that the users found that the recipes suggested (even though they had high quality) did not match the customer's diet and needs\\\n",
    "Now we have a system of likes and dislikes for the recipes and a new user interface, where the users can enter information about what they want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 2: Data Understanding\n",
    "\n",
    "Assess the data quality and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bc/gcpryyxs74146gl1yfg2msyw0000gn/T/ipykernel_44973/1456728697.py:5: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  reviews = pd.read_csv(\"reviews.csv\")\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "diet = pd.read_csv(\"diet.csv\")\n",
    "recipes = pd.read_csv(\"recipes.csv\")\n",
    "requests = pd.read_csv(\"requests.csv\")\n",
    "reviews = pd.read_csv(\"reviews.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "have a look at the data and its attributes \\\n",
    "check if columns are properly named \\\n",
    "general overview over data, check for missing values, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diet pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "diet[\"Diet\"] = diet[\"Diet\"].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Recipes table pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RecipeId</th>\n",
       "      <th>Name</th>\n",
       "      <th>CookTime</th>\n",
       "      <th>PrepTime</th>\n",
       "      <th>RecipeCategory</th>\n",
       "      <th>RecipeIngredientQuantities</th>\n",
       "      <th>RecipeIngredientParts</th>\n",
       "      <th>Calories</th>\n",
       "      <th>FatContent</th>\n",
       "      <th>SaturatedFatContent</th>\n",
       "      <th>CholesterolContent</th>\n",
       "      <th>SodiumContent</th>\n",
       "      <th>CarbohydrateContent</th>\n",
       "      <th>FiberContent</th>\n",
       "      <th>SugarContent</th>\n",
       "      <th>ProteinContent</th>\n",
       "      <th>RecipeServings</th>\n",
       "      <th>RecipeYield</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73440</td>\n",
       "      <td>Bow Ties With Broccoli Pesto</td>\n",
       "      <td>0</td>\n",
       "      <td>1800</td>\n",
       "      <td>Other</td>\n",
       "      <td>[6,  2,  1 1/2,  1/4,  1/2,  4,  1 1/2,  1 1/2...</td>\n",
       "      <td>[hazelnuts,  broccoli florets,  fresh parsley ...</td>\n",
       "      <td>241.3</td>\n",
       "      <td>10.1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.1</td>\n",
       "      <td>31.8</td>\n",
       "      <td>2.3</td>\n",
       "      <td>1.4</td>\n",
       "      <td>6.7</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>365718</td>\n",
       "      <td>Cashew-chutney Rice</td>\n",
       "      <td>3600</td>\n",
       "      <td>600</td>\n",
       "      <td>Other</td>\n",
       "      <td>[1,  3/4,  6,  5,  2,  1,  2]</td>\n",
       "      <td>[celery,  onion,  butter,  chicken broth,  lon...</td>\n",
       "      <td>370.8</td>\n",
       "      <td>17.5</td>\n",
       "      <td>7.2</td>\n",
       "      <td>22.9</td>\n",
       "      <td>553.3</td>\n",
       "      <td>44.3</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>9.4</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>141757</td>\n",
       "      <td>Copycat Taco Bell Nacho Fries BellGrande</td>\n",
       "      <td>3600</td>\n",
       "      <td>2700</td>\n",
       "      <td>Other</td>\n",
       "      <td>[3,  1/2,  1,  1,  3,  2,  1,  2 1/2,  2,  1, ...</td>\n",
       "      <td>[Copycat Taco Bell Seasoned Beef,  yellow onio...</td>\n",
       "      <td>377.6</td>\n",
       "      <td>20.9</td>\n",
       "      <td>10.5</td>\n",
       "      <td>45.7</td>\n",
       "      <td>1501.8</td>\n",
       "      <td>36.6</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.1</td>\n",
       "      <td>12.9</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>280351</td>\n",
       "      <td>Slow Cooker Jalapeno Cheddar Cheese Soup</td>\n",
       "      <td>18000</td>\n",
       "      <td>1800</td>\n",
       "      <td>Other</td>\n",
       "      <td>[2,  1,  2,  2,  1,  1,  1/8,  1/4,  1,  4,  3...</td>\n",
       "      <td>[unsalted butter,  yellow onion,  carrots,  ga...</td>\n",
       "      <td>282.8</td>\n",
       "      <td>16.5</td>\n",
       "      <td>10.3</td>\n",
       "      <td>50.5</td>\n",
       "      <td>630.2</td>\n",
       "      <td>22.8</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>11.7</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>180505</td>\n",
       "      <td>Cool &amp; Crisp Citrus Chiffon Pie</td>\n",
       "      <td>3600</td>\n",
       "      <td>1800</td>\n",
       "      <td>Other</td>\n",
       "      <td>[1,  1/4,  1/2,  1/2,  1,  1/2,  4,  4,  1/2, ...</td>\n",
       "      <td>[unflavored gelatin,  water,  sugar,  lemon,  ...</td>\n",
       "      <td>257.5</td>\n",
       "      <td>8.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>110.7</td>\n",
       "      <td>160.9</td>\n",
       "      <td>39.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>30.2</td>\n",
       "      <td>6.3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RecipeId                                      Name  CookTime  PrepTime  \\\n",
       "0     73440              Bow Ties With Broccoli Pesto         0      1800   \n",
       "1    365718                       Cashew-chutney Rice      3600       600   \n",
       "2    141757  Copycat Taco Bell Nacho Fries BellGrande      3600      2700   \n",
       "3    280351  Slow Cooker Jalapeno Cheddar Cheese Soup     18000      1800   \n",
       "4    180505           Cool & Crisp Citrus Chiffon Pie      3600      1800   \n",
       "\n",
       "  RecipeCategory                         RecipeIngredientQuantities  \\\n",
       "0          Other  [6,  2,  1 1/2,  1/4,  1/2,  4,  1 1/2,  1 1/2...   \n",
       "1          Other                      [1,  3/4,  6,  5,  2,  1,  2]   \n",
       "2          Other  [3,  1/2,  1,  1,  3,  2,  1,  2 1/2,  2,  1, ...   \n",
       "3          Other  [2,  1,  2,  2,  1,  1,  1/8,  1/4,  1,  4,  3...   \n",
       "4          Other  [1,  1/4,  1/2,  1/2,  1,  1/2,  4,  4,  1/2, ...   \n",
       "\n",
       "                               RecipeIngredientParts  Calories  FatContent  \\\n",
       "0  [hazelnuts,  broccoli florets,  fresh parsley ...     241.3        10.1   \n",
       "1  [celery,  onion,  butter,  chicken broth,  lon...     370.8        17.5   \n",
       "2  [Copycat Taco Bell Seasoned Beef,  yellow onio...     377.6        20.9   \n",
       "3  [unsalted butter,  yellow onion,  carrots,  ga...     282.8        16.5   \n",
       "4  [unflavored gelatin,  water,  sugar,  lemon,  ...     257.5         8.6   \n",
       "\n",
       "   SaturatedFatContent  CholesterolContent  SodiumContent  \\\n",
       "0                  1.2                 0.0           13.1   \n",
       "1                  7.2                22.9          553.3   \n",
       "2                 10.5                45.7         1501.8   \n",
       "3                 10.3                50.5          630.2   \n",
       "4                  2.4               110.7          160.9   \n",
       "\n",
       "   CarbohydrateContent  FiberContent  SugarContent  ProteinContent  \\\n",
       "0                 31.8           2.3           1.4             6.7   \n",
       "1                 44.3           1.6           2.2             9.4   \n",
       "2                 36.6           3.8           6.1            12.9   \n",
       "3                 22.8           2.3           2.7            11.7   \n",
       "4                 39.8           0.4          30.2             6.3   \n",
       "\n",
       "   RecipeServings RecipeYield  \n",
       "0             9.0         NaN  \n",
       "1             8.0         NaN  \n",
       "2             8.0         NaN  \n",
       "3             6.0         NaN  \n",
       "4             6.0         NaN  "
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change types of column\n",
    "def refactorIngredients(ingredients):\n",
    "    if ingredients == \"character(0)\":\n",
    "        return []\n",
    "    ingredients = ingredients.replace(\"\\\\\", '').replace(\"\\\"\", '').replace('c(','').replace(')', '')\n",
    "    ingredients = ingredients.split(\",\")\n",
    "    return ingredients\n",
    "\n",
    "recipes[\"RecipeIngredientQuantities\"] = recipes[\"RecipeIngredientQuantities\"].apply(lambda x: refactorIngredients(x))\n",
    "recipes[\"RecipeIngredientParts\"] = recipes[\"RecipeIngredientParts\"].apply(lambda x: refactorIngredients(x))\n",
    "\n",
    "recipes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RecipeId</th>\n",
       "      <th>Name</th>\n",
       "      <th>CookTime</th>\n",
       "      <th>PrepTime</th>\n",
       "      <th>Calories</th>\n",
       "      <th>FatContent</th>\n",
       "      <th>SaturatedFatContent</th>\n",
       "      <th>CholesterolContent</th>\n",
       "      <th>SodiumContent</th>\n",
       "      <th>CarbohydrateContent</th>\n",
       "      <th>FiberContent</th>\n",
       "      <th>SugarContent</th>\n",
       "      <th>ProteinContent</th>\n",
       "      <th>RecipeDiet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73440</td>\n",
       "      <td>Bow Ties With Broccoli Pesto</td>\n",
       "      <td>0</td>\n",
       "      <td>1800</td>\n",
       "      <td>241.3</td>\n",
       "      <td>10.1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.1</td>\n",
       "      <td>31.8</td>\n",
       "      <td>2.3</td>\n",
       "      <td>1.4</td>\n",
       "      <td>6.7</td>\n",
       "      <td>Vegan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>365718</td>\n",
       "      <td>Cashew-chutney Rice</td>\n",
       "      <td>3600</td>\n",
       "      <td>600</td>\n",
       "      <td>370.8</td>\n",
       "      <td>17.5</td>\n",
       "      <td>7.2</td>\n",
       "      <td>22.9</td>\n",
       "      <td>553.3</td>\n",
       "      <td>44.3</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>9.4</td>\n",
       "      <td>Omnivore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>141757</td>\n",
       "      <td>Copycat Taco Bell Nacho Fries BellGrande</td>\n",
       "      <td>3600</td>\n",
       "      <td>2700</td>\n",
       "      <td>377.6</td>\n",
       "      <td>20.9</td>\n",
       "      <td>10.5</td>\n",
       "      <td>45.7</td>\n",
       "      <td>1501.8</td>\n",
       "      <td>36.6</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.1</td>\n",
       "      <td>12.9</td>\n",
       "      <td>Omnivore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>280351</td>\n",
       "      <td>Slow Cooker Jalapeno Cheddar Cheese Soup</td>\n",
       "      <td>18000</td>\n",
       "      <td>1800</td>\n",
       "      <td>282.8</td>\n",
       "      <td>16.5</td>\n",
       "      <td>10.3</td>\n",
       "      <td>50.5</td>\n",
       "      <td>630.2</td>\n",
       "      <td>22.8</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>11.7</td>\n",
       "      <td>Omnivore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>180505</td>\n",
       "      <td>Cool &amp; Crisp Citrus Chiffon Pie</td>\n",
       "      <td>3600</td>\n",
       "      <td>1800</td>\n",
       "      <td>257.5</td>\n",
       "      <td>8.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>110.7</td>\n",
       "      <td>160.9</td>\n",
       "      <td>39.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>30.2</td>\n",
       "      <td>6.3</td>\n",
       "      <td>Vegetarian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75599</th>\n",
       "      <td>253577</td>\n",
       "      <td>Frijoles Negros- Crock Pot Mexican Black Beans</td>\n",
       "      <td>43200</td>\n",
       "      <td>28800</td>\n",
       "      <td>121.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1175.1</td>\n",
       "      <td>22.2</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.6</td>\n",
       "      <td>7.9</td>\n",
       "      <td>Vegan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75600</th>\n",
       "      <td>267827</td>\n",
       "      <td>Moose Moussaka</td>\n",
       "      <td>3600</td>\n",
       "      <td>2700</td>\n",
       "      <td>652.2</td>\n",
       "      <td>25.8</td>\n",
       "      <td>10.7</td>\n",
       "      <td>197.9</td>\n",
       "      <td>435.5</td>\n",
       "      <td>51.9</td>\n",
       "      <td>7.5</td>\n",
       "      <td>7.2</td>\n",
       "      <td>50.1</td>\n",
       "      <td>Vegetarian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75601</th>\n",
       "      <td>266983</td>\n",
       "      <td>Cantonese Pepper Steak for Two (Or More)</td>\n",
       "      <td>1800</td>\n",
       "      <td>900</td>\n",
       "      <td>223.9</td>\n",
       "      <td>9.2</td>\n",
       "      <td>3.6</td>\n",
       "      <td>78.3</td>\n",
       "      <td>725.9</td>\n",
       "      <td>7.3</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.7</td>\n",
       "      <td>26.7</td>\n",
       "      <td>Omnivore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75602</th>\n",
       "      <td>253739</td>\n",
       "      <td>Coconut Cream Cooler</td>\n",
       "      <td>300</td>\n",
       "      <td>120</td>\n",
       "      <td>2229.8</td>\n",
       "      <td>80.3</td>\n",
       "      <td>69.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>294.7</td>\n",
       "      <td>369.0</td>\n",
       "      <td>15.7</td>\n",
       "      <td>317.9</td>\n",
       "      <td>26.7</td>\n",
       "      <td>Vegan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75603</th>\n",
       "      <td>78171</td>\n",
       "      <td>Cheater Risotto</td>\n",
       "      <td>960</td>\n",
       "      <td>600</td>\n",
       "      <td>654.1</td>\n",
       "      <td>13.8</td>\n",
       "      <td>6.9</td>\n",
       "      <td>34.6</td>\n",
       "      <td>1114.0</td>\n",
       "      <td>92.2</td>\n",
       "      <td>3.9</td>\n",
       "      <td>4.2</td>\n",
       "      <td>21.8</td>\n",
       "      <td>Omnivore</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75604 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       RecipeId                                            Name  CookTime  \\\n",
       "0         73440                    Bow Ties With Broccoli Pesto         0   \n",
       "1        365718                             Cashew-chutney Rice      3600   \n",
       "2        141757        Copycat Taco Bell Nacho Fries BellGrande      3600   \n",
       "3        280351        Slow Cooker Jalapeno Cheddar Cheese Soup     18000   \n",
       "4        180505                 Cool & Crisp Citrus Chiffon Pie      3600   \n",
       "...         ...                                             ...       ...   \n",
       "75599    253577  Frijoles Negros- Crock Pot Mexican Black Beans     43200   \n",
       "75600    267827                                  Moose Moussaka      3600   \n",
       "75601    266983        Cantonese Pepper Steak for Two (Or More)      1800   \n",
       "75602    253739                            Coconut Cream Cooler       300   \n",
       "75603     78171                                 Cheater Risotto       960   \n",
       "\n",
       "       PrepTime  Calories  FatContent  SaturatedFatContent  \\\n",
       "0          1800     241.3        10.1                  1.2   \n",
       "1           600     370.8        17.5                  7.2   \n",
       "2          2700     377.6        20.9                 10.5   \n",
       "3          1800     282.8        16.5                 10.3   \n",
       "4          1800     257.5         8.6                  2.4   \n",
       "...         ...       ...         ...                  ...   \n",
       "75599     28800     121.5         0.5                  0.1   \n",
       "75600      2700     652.2        25.8                 10.7   \n",
       "75601       900     223.9         9.2                  3.6   \n",
       "75602       120    2229.8        80.3                 69.3   \n",
       "75603       600     654.1        13.8                  6.9   \n",
       "\n",
       "       CholesterolContent  SodiumContent  CarbohydrateContent  FiberContent  \\\n",
       "0                     0.0           13.1                 31.8           2.3   \n",
       "1                    22.9          553.3                 44.3           1.6   \n",
       "2                    45.7         1501.8                 36.6           3.8   \n",
       "3                    50.5          630.2                 22.8           2.3   \n",
       "4                   110.7          160.9                 39.8           0.4   \n",
       "...                   ...            ...                  ...           ...   \n",
       "75599                 0.0         1175.1                 22.2           7.8   \n",
       "75600               197.9          435.5                 51.9           7.5   \n",
       "75601                78.3          725.9                  7.3           1.1   \n",
       "75602                 0.0          294.7                369.0          15.7   \n",
       "75603                34.6         1114.0                 92.2           3.9   \n",
       "\n",
       "       SugarContent  ProteinContent  RecipeDiet  \n",
       "0               1.4             6.7       Vegan  \n",
       "1               2.2             9.4    Omnivore  \n",
       "2               6.1            12.9    Omnivore  \n",
       "3               2.7            11.7    Omnivore  \n",
       "4              30.2             6.3  Vegetarian  \n",
       "...             ...             ...         ...  \n",
       "75599           0.6             7.9       Vegan  \n",
       "75600           7.2            50.1  Vegetarian  \n",
       "75601           1.7            26.7    Omnivore  \n",
       "75602         317.9            26.7       Vegan  \n",
       "75603           4.2            21.8    Omnivore  \n",
       "\n",
       "[75604 rows x 14 columns]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determines if recipe is veggie, vegan or omnivore\n",
    "def categorizeRecipe(ingredients):\n",
    "    meat_derivates = [\"pork\", \"beef\", \"meat\", \"fish\", \"tuna\", \"chicken\", \"squid\", \"schrimp\", \"trout\", \"mussels\", \n",
    "                      \"fillet\", \"lamb\", \"scallops\", \"sardine\", \"salmon\", \"lobster\", \"steak\", \"bacon\", \"ham\", \"oyster\"]\n",
    "    animal_derivates = [\"milk\", \"egg\", \"honey\", \"gelatin\", \"butter\", \"mayonnaise\", \"cheese\", \"margarine\", \n",
    "                    \" heavy\", \"yogurt\", \"pudding\", \"shortening\", \"ice cream\", \"chocolate\", \"alfredo\", \"Miracle Whip\", \"half-and-half\"]\n",
    "    vegan_exclusions = [\"substitute\", \"peanut\", \"apple\", \"vegan\", \"soymilk\"]\n",
    "    vegan = True\n",
    "    for ingredient in ingredients:\n",
    "        if any(word in ingredient.lower() for word in meat_derivates):\n",
    "            return \"Omnivore\"\n",
    "        if ingredient in vegan_exclusions:\n",
    "            continue\n",
    "        if any(word in ingredient.lower() for word in animal_derivates):\n",
    "            vegan = False\n",
    "    if vegan: \n",
    "        return \"Vegan\"\n",
    "    else: \n",
    "        return \"Vegetarian\"\n",
    "\n",
    "recipes[\"RecipeDiet\"] = recipes[\"RecipeIngredientParts\"].apply(lambda x: categorizeRecipe(x))\n",
    "recipes['RecipeDiet'] = recipes['RecipeDiet'].astype('category')\n",
    "\n",
    "# Create another table \"recipe extra info\" columns category, ingredient quatities, parts\n",
    "selected_columns = ['RecipeCategory', 'RecipeIngredientQuantities', 'RecipeIngredientParts', 'RecipeServings', 'RecipeYield']\n",
    "recipe_extra_info = recipes[selected_columns]\n",
    "recipes = recipes.drop(columns=selected_columns)\n",
    "\n",
    "recipes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.info()\n",
    "# no missing values: GOOD!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming the columns\n",
    "requests = requests.rename(columns={\"HighCalories\": \"Calories\", \"HighProtein\":\"Protein\", \"LowFat\": \"Fat\", \"LowSugar\": \"Sugar\", \"HighFiber\":\"Fiber\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AuthorId</th>\n",
       "      <th>RecipeId</th>\n",
       "      <th>Time</th>\n",
       "      <th>Calories</th>\n",
       "      <th>Protein</th>\n",
       "      <th>Fat</th>\n",
       "      <th>Sugar</th>\n",
       "      <th>Fiber</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001012259B</td>\n",
       "      <td>73440</td>\n",
       "      <td>1799.950949</td>\n",
       "      <td>0</td>\n",
       "      <td>Indifferent</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>437641B</td>\n",
       "      <td>365718</td>\n",
       "      <td>4201.820980</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Indifferent</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1803340263D</td>\n",
       "      <td>141757</td>\n",
       "      <td>6299.861496</td>\n",
       "      <td>0</td>\n",
       "      <td>Indifferent</td>\n",
       "      <td>0</td>\n",
       "      <td>Indifferent</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>854048B</td>\n",
       "      <td>280351</td>\n",
       "      <td>19801.365796</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2277685E</td>\n",
       "      <td>180505</td>\n",
       "      <td>5400.093457</td>\n",
       "      <td>0</td>\n",
       "      <td>Indifferent</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140190</th>\n",
       "      <td>163793B</td>\n",
       "      <td>78171</td>\n",
       "      <td>1560.649725</td>\n",
       "      <td>0</td>\n",
       "      <td>Indifferent</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140191</th>\n",
       "      <td>33888B</td>\n",
       "      <td>333262</td>\n",
       "      <td>1502.011466</td>\n",
       "      <td>1</td>\n",
       "      <td>Indifferent</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140192</th>\n",
       "      <td>401942C</td>\n",
       "      <td>49200</td>\n",
       "      <td>5999.274269</td>\n",
       "      <td>0</td>\n",
       "      <td>Indifferent</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140193</th>\n",
       "      <td>346866B</td>\n",
       "      <td>214815</td>\n",
       "      <td>899.523513</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Indifferent</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140194</th>\n",
       "      <td>1786859E</td>\n",
       "      <td>117923</td>\n",
       "      <td>7199.637837</td>\n",
       "      <td>1</td>\n",
       "      <td>Indifferent</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140195 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           AuthorId  RecipeId          Time Calories      Protein Fat  \\\n",
       "0       2001012259B     73440   1799.950949        0  Indifferent   1   \n",
       "1           437641B    365718   4201.820980        0            1   1   \n",
       "2       1803340263D    141757   6299.861496        0  Indifferent   0   \n",
       "3           854048B    280351  19801.365796        0            1   0   \n",
       "4          2277685E    180505   5400.093457        0  Indifferent   1   \n",
       "...             ...       ...           ...      ...          ...  ..   \n",
       "140190      163793B     78171   1560.649725        0  Indifferent   1   \n",
       "140191       33888B    333262   1502.011466        1  Indifferent   0   \n",
       "140192      401942C     49200   5999.274269        0  Indifferent   1   \n",
       "140193      346866B    214815    899.523513        0            1   0   \n",
       "140194     1786859E    117923   7199.637837        1  Indifferent   1   \n",
       "\n",
       "              Sugar Fiber  \n",
       "0                 1     0  \n",
       "1       Indifferent     1  \n",
       "2       Indifferent     0  \n",
       "3                 1     1  \n",
       "4                 1     0  \n",
       "...             ...   ...  \n",
       "140190            1     1  \n",
       "140191            1     0  \n",
       "140192            1     1  \n",
       "140193  Indifferent     1  \n",
       "140194            1     1  \n",
       "\n",
       "[140195 rows x 8 columns]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# standardizing column Calorie to the same format\n",
    "requests[\"Calories\"] = requests[\"Calories\"].astype(\"int\")\n",
    "\n",
    "# standardizing column Protein Yes->1\n",
    "requests[\"Protein\"] = requests[\"Protein\"].replace(\"Yes\",\"1\")\n",
    "\n",
    "# changing 0 -> 1 in column Sugar \n",
    "requests[\"Sugar\"] = requests[\"Sugar\"].replace(\"0\",\"1\")\n",
    "\n",
    "# changing 0 -> 1 and 1 -> 0  column Fat\n",
    "#requests[\"Fat\"] = requests[\"Fat\"].replace({1 : 0, 0 : 1})\n",
    "requests[\"Fat\"] = 1 - requests[\"Fat\"]\n",
    "\n",
    "# transforming macronutrients columns -> categories \n",
    "requests[[\"Calories\", \"Protein\", \"Fiber\", \"Sugar\", \"Fat\"]] = requests[[\"Calories\", \"Protein\", \"Fiber\", \"Sugar\", \"Fat\"]].astype(\"category\")\n",
    "\n",
    "requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "df_grouped_by_class = df.groupby(by=\"variety\")\n",
    "\n",
    "df_setosa = df_grouped_by_class.get_group(\"Setosa\")\n",
    "df_versicolor = df_grouped_by_class.get_group(\"Versicolor\")\n",
    "df_virginica = df_grouped_by_class.get_group(\"Virginica\")\n",
    "\n",
    "class_labels = {\n",
    "    \"Setosa\" : {\n",
    "        \"color\" : \"blue\",\n",
    "        \"data\" : df_setosa\n",
    "    },\n",
    "    \"Versicolor\" : {\n",
    "        \"color\" : \"green\",\n",
    "        \"data\" : df_versicolor\n",
    "    },\n",
    "    \"Virginica\" : {\n",
    "        \"color\" : \"red\",\n",
    "        \"data\" : df_virginica\n",
    "    }\n",
    "}\n",
    "\n",
    "for class_i in class_labels:\n",
    "    class_color = class_labels[class_i][\"color\"]\n",
    "    class_df = class_labels[class_i][\"data\"]\n",
    "    p = sns.pairplot(class_df, diag_kind=\"hist\", diag_kws={\"color\" : class_color}, plot_kws={\"color\" : class_color, \"label\" : class_i})\n",
    "    p.fig.suptitle(class_i, y=1.0, size=15)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# We can also leverage the dataprep package to get a nice summary report\n",
    "report = sv.analyze(df)\n",
    "report.show_notebook()\n",
    "\n",
    "# We can also leverage the yadata_profiling package to get a nice summary report\n",
    "profile = ProfileReport(df, title=\"Iris Data - Summary Report\")\n",
    "profile\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 3: Data Preparation\n",
    "\n",
    "The goal is assure data quality: includes removing wrong/corrupt \n",
    "data entries and making sure the entries are standardized, e.g. enforcing certain encodings. \n",
    "Then transforms the data in order to make it suitable for the modelling step. This includes scaling, dimensionality\n",
    "reduction, data augmentation, outlier removal, etc.\\\n",
    " \\\n",
    "In practise, this will rarely be the case. On average, this step takes up to **80%** of \n",
    "the time of the whole project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To do: transform categorical feature into categorical variables (exemplo df[\"variety\"] = df[\"variety\"].astype(\"category\"))\n",
    "# fill/remove/change missing/corrupt values\n",
    "\n",
    "# To do: ver se precisamos standardize alguma feature (exemplo na celula seguinte com o StandardScaler), se precisamos imputar valores em registros com valores nulos, \n",
    "# se precisamos lidar com outliers, se precisamos usar alguma estretégia de redução de dimensionalidade (tipo PCA na próxima celula)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# data scaling\n",
    "transform_scaler = StandardScaler()\n",
    "\n",
    "# dimensionality reduction\n",
    "transform_pca = PCA()\n",
    "\n",
    "# value imputing\n",
    "\n",
    "# outlier detection/removal\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join das 4 tabelas\n",
    "- há users na tabela \"diet\" que nao estao na tabela \"reviews\" -- Ok!\n",
    "- match perfeito de recipeid and authorid entre requests e reviews -- Otimo!\n",
    "- todas as receitas de \"recipes\" estao sendo mostradas para pelo menos um usuario -- Ok!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tabelas: diet, requests, reviews, recipes\n",
    "dietrequestsmerged = diet.merge(requests, on = [\"AuthorId\"])\n",
    "dietrequestsreviewsmerged = dietrequestsmerged.merge(reviews, on = [\"AuthorId\", \"RecipeId\"])\n",
    "dietrequestsreviewsmerged = dietrequestsreviewsmerged.rename(columns={\"Calories\" : \"Requested_Calories\"})\n",
    "mergedtables = dietrequestsreviewsmerged.merge(recipes, on = [\"RecipeId\"])\n",
    "mergedtables = mergedtables.rename(columns={\"Calories\" : \"Recipe_Calories\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 97381 entries, 0 to 140194\n",
      "Data columns (total 26 columns):\n",
      " #   Column               Non-Null Count  Dtype   \n",
      "---  ------               --------------  -----   \n",
      " 0   AuthorId             97381 non-null  object  \n",
      " 1   Diet                 97381 non-null  category\n",
      " 2   Age                  97381 non-null  int64   \n",
      " 3   RecipeId             97381 non-null  int64   \n",
      " 4   Time                 97381 non-null  float64 \n",
      " 5   Requested_Calories   97381 non-null  category\n",
      " 6   Protein              97381 non-null  category\n",
      " 7   Fat                  97381 non-null  category\n",
      " 8   Sugar                97381 non-null  category\n",
      " 9   Fiber                97381 non-null  category\n",
      " 10  Rating               53351 non-null  float64 \n",
      " 11  Like                 97381 non-null  object  \n",
      " 12  TestSetId            0 non-null      float64 \n",
      " 13  Name                 97381 non-null  object  \n",
      " 14  CookTime             97381 non-null  int64   \n",
      " 15  PrepTime             97381 non-null  int64   \n",
      " 16  Recipe_Calories      97381 non-null  float64 \n",
      " 17  FatContent           97381 non-null  float64 \n",
      " 18  SaturatedFatContent  97381 non-null  float64 \n",
      " 19  CholesterolContent   97381 non-null  float64 \n",
      " 20  SodiumContent        97381 non-null  float64 \n",
      " 21  CarbohydrateContent  97381 non-null  float64 \n",
      " 22  FiberContent         97381 non-null  float64 \n",
      " 23  SugarContent         97381 non-null  float64 \n",
      " 24  ProteinContent       97381 non-null  float64 \n",
      " 25  RecipeDiet           97381 non-null  category\n",
      "dtypes: category(7), float64(12), int64(4), object(3)\n",
      "memory usage: 15.5+ MB\n"
     ]
    }
   ],
   "source": [
    "submissiondataset = mergedtables[mergedtables[\"Like\"].isna()] #com Null na coluna Like\n",
    "trainandtestdataset = mergedtables[mergedtables[\"Like\"].notna()] #sem Null na coluna Like\n",
    "\n",
    "trainandtestdataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split our data set into *train* and *test* data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do: ver se vamos usar um split para validação, ou usar cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 97381 entries, 0 to 140194\n",
      "Data columns (total 31 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   Age                    97381 non-null  int64  \n",
      " 1   Time                   97381 non-null  float64\n",
      " 2   Rating                 53351 non-null  float64\n",
      " 3   Like                   97381 non-null  object \n",
      " 4   CookTime               97381 non-null  int64  \n",
      " 5   PrepTime               97381 non-null  int64  \n",
      " 6   Recipe_Calories        97381 non-null  float64\n",
      " 7   FatContent             97381 non-null  float64\n",
      " 8   SaturatedFatContent    97381 non-null  float64\n",
      " 9   CholesterolContent     97381 non-null  float64\n",
      " 10  SodiumContent          97381 non-null  float64\n",
      " 11  CarbohydrateContent    97381 non-null  float64\n",
      " 12  FiberContent           97381 non-null  float64\n",
      " 13  SugarContent           97381 non-null  float64\n",
      " 14  ProteinContent         97381 non-null  float64\n",
      " 15  Diet_Omnivore          97381 non-null  bool   \n",
      " 16  Diet_Vegan             97381 non-null  bool   \n",
      " 17  Diet_Vegetarian        97381 non-null  bool   \n",
      " 18  RecipeDiet_Omnivore    97381 non-null  bool   \n",
      " 19  RecipeDiet_Vegan       97381 non-null  bool   \n",
      " 20  RecipeDiet_Vegetarian  97381 non-null  bool   \n",
      " 21  Requested_Calories_0   97381 non-null  bool   \n",
      " 22  Requested_Calories_1   97381 non-null  bool   \n",
      " 23  Protein_1              97381 non-null  bool   \n",
      " 24  Protein_Indifferent    97381 non-null  bool   \n",
      " 25  Fat_0                  97381 non-null  bool   \n",
      " 26  Fat_1                  97381 non-null  bool   \n",
      " 27  Sugar_1                97381 non-null  bool   \n",
      " 28  Sugar_Indifferent      97381 non-null  bool   \n",
      " 29  Fiber_0                97381 non-null  bool   \n",
      " 30  Fiber_1                97381 non-null  bool   \n",
      "dtypes: bool(16), float64(11), int64(3), object(1)\n",
      "memory usage: 13.4+ MB\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trainandtestdataset = trainandtestdataset.drop(columns=['AuthorId', 'RecipeId', 'TestSetId', 'Name'])\n",
    "\n",
    "categorical_values = ['Diet', 'RecipeDiet', 'Requested_Calories', 'Protein', 'Fat', 'Sugar', 'Fiber']\n",
    "\n",
    "for column in categorical_values:\n",
    "    new_data = pd.get_dummies(trainandtestdataset[column], prefix=column)\n",
    "    trainandtestdataset = pd.concat([trainandtestdataset, new_data], axis=1)\n",
    "    \n",
    "trainandtestdataset = trainandtestdataset.drop(columns=categorical_values)\n",
    "trainandtestdataset.to_csv(\"trainandtestdataset.csv\")\n",
    "    \n",
    "trainandtestdataset.head()\n",
    "\n",
    "X_features = trainandtestdataset.drop(columns=\"Like\")\n",
    "Y_classes = trainandtestdataset[\"Like\"]\n",
    "Y_classes = Y_classes.astype('category')\n",
    "\n",
    "trainandtestdataset.info()\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_features, Y_classes,\n",
    "                                                    test_size=0.2, \n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=seed) # for reproducibility\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- X_train: 77.904 rows × 24 columns\n",
    "- Y_train: 77.904 rows\n",
    "- X_test: 19.477 rows × 24 columns\n",
    "- Y_test: 19.477 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 4: Modeling\n",
    "\n",
    "In this phase, the model is trained and tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nRandomForestClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[178], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[1;32m      4\u001b[0m random_forest \u001b[38;5;241m=\u001b[39m RandomForestClassifier()\n\u001b[0;32m----> 6\u001b[0m \u001b[43mrandom_forest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m Y_pred \u001b[38;5;241m=\u001b[39m random_forest\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m     10\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(Y_test, Y_pred)\n",
      "File \u001b[0;32m~/Desktop/analytics_cup/.venv/lib/python3.9/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/analytics_cup/.venv/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:348\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(y):\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 348\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    352\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[0;32m~/Desktop/analytics_cup/.venv/lib/python3.9/site-packages/sklearn/base.py:622\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    620\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 622\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    623\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/Desktop/analytics_cup/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:1146\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1142\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1143\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1144\u001b[0m     )\n\u001b[0;32m-> 1146\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1162\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m   1164\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m~/Desktop/analytics_cup/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:957\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    951\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    952\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    953\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    954\u001b[0m         )\n\u001b[1;32m    956\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 957\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    965\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m~/Desktop/analytics_cup/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:122\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/analytics_cup/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:171\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    170\u001b[0m     )\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nRandomForestClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "random_forest = RandomForestClassifier()\n",
    "\n",
    "random_forest.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = random_forest.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, you want to find the best classifier. As candidates, consider\n",
    "#   1. LogisticRegression\n",
    "#   2. RandomForestClassifier\n",
    "#   3. other algorithms from sklearn (easy to add)\n",
    "#   4. custom algorithms (more difficult to implement)\n",
    "    \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model_logistic_regression = LogisticRegression(max_iter=30)\n",
    "model_random_forest = RandomForestClassifier()\n",
    "model_gradient_boosting = GradientBoostingClassifier()\n",
    "\n",
    "# train the models\n",
    "pipeline = Pipeline(steps=[(\"scaler\", transform_scaler), \n",
    "                           (\"pca\", transform_pca),\n",
    "                           (\"model\", None)])\n",
    "\n",
    "parameter_grid_preprocessing = {\n",
    "  \"pca__n_components\" : [1, 2, 3, 4],\n",
    "}\n",
    "\n",
    "parameter_grid_logistic_regression = {\n",
    "  \"model\" : [model_logistic_regression],\n",
    "  \"model__C\" : [0.1, 1, 10],  # inverse regularization strength\n",
    "}\n",
    "\n",
    "parameter_grid_gradient_boosting = {\n",
    "  \"model\" : [model_gradient_boosting],\n",
    "  \"model__n_estimators\" : [10, 20, 30]\n",
    "}\n",
    "\n",
    "parameter_grid_random_forest = {\n",
    "  \"model\" : [model_random_forest],\n",
    "  \"model__n_estimators\" : [10, 20, 50],  # number of max trees in the forest\n",
    "  \"model__max_depth\" : [2, 3, 4],\n",
    "}\n",
    "\n",
    "meta_parameter_grid = [parameter_grid_logistic_regression,\n",
    "                       parameter_grid_random_forest,\n",
    "                       parameter_grid_gradient_boosting]\n",
    "\n",
    "meta_parameter_grid = [{**parameter_grid_preprocessing, **model_grid}\n",
    "                       for model_grid in meta_parameter_grid]\n",
    "\n",
    "search = GridSearchCV(pipeline,\n",
    "                      meta_parameter_grid, \n",
    "                      scoring=\"balanced_accuracy\",\n",
    "                      n_jobs=2, \n",
    "                      cv=5,  # number of folds for cross-validation \n",
    "                      error_score=\"raise\"\n",
    ")\n",
    "\n",
    "# here, the actual training and grid search happens\n",
    "search.fit(X_train, Y_train.values.ravel())\n",
    "\n",
    "print(\"best parameter:\", search.best_params_ ,\"(CV score=%0.3f)\" % search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Evaluation\n",
    "\n",
    "Once the appropriate models are chosen, they are evaluated on the test set. For\n",
    "this, different evaluation metrics can be used. Furthermore, this step is where\n",
    "the models and their predictions are analyzed resp. different properties, including\n",
    "feature importance, robustness to outliers, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate performance of model on test set\n",
    "print(\"Score on test set:\", search.score(X_test, Y_test.values.ravel()))\n",
    "\n",
    "# contingency table\n",
    "ct = pd.crosstab(search.best_estimator_.predict(X_test), Y_test.values.ravel(),\n",
    "                 rownames=[\"pred\"], colnames=[\"true\"])\n",
    "print(ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (optional, if you're curious) \n",
    "# for a detailed look on the performance of the different models\n",
    "def get_search_score_overview():\n",
    "  for c,s in zip(search.cv_results_[\"params\"],search.cv_results_[\"mean_test_score\"]):\n",
    "      print(c, s)\n",
    "\n",
    "print(get_search_score_overview())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretability\n",
    "\n",
    "##### Disclaimer: This only works if shap is installed.\n",
    "\n",
    "In addition to models and their predictions, it is often important to understand _why_ a model makes certain predictions. \n",
    "There is a lot of literature on how this can be achieved (explainability), but we will only show the use of Shapley values\n",
    "using the python module \"shap\", which is a combination of Shapley values and LIME. \n",
    "You can find more information on this topic [here](https://christophm.github.io/interpretable-ml-book/shap.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume random forest model\n",
    "model = RandomForestClassifier(n_estimators=10, random_state=seed)\n",
    "model.fit(X_train, Y_train.values.ravel())\n",
    "\n",
    "# compute shapley values\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_train)\n",
    "shap_interaction_values = explainer.shap_interaction_values(X_train)\n",
    "\n",
    "expected_value = explainer.expected_value\n",
    "print(expected_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class dependent plots of shapley values for each feature\n",
    "for i,c in enumerate(df.variety.unique()):\n",
    "    shap.summary_plot(shap_values[i], X_train, show=False)\n",
    "    plt.title(\"Shapley values for \"+str(c))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the computed SHAP values, we can interpret that the *petal.width* has a positive impact on the output of the model \n",
    "if the feature value is moderate. For high aand low values, the impact is negative. The same observation\n",
    "holds for *petal.length*. Besides, the impact of the *sepal.length* and *sepal.width* features are rather low. By impact on a \n",
    "the target, we model the probability that we classify that target. Thus, if *petal.width* is high, it is more likely\n",
    "that we classify the data point as Versicolor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Deployment\n",
    "\n",
    "Now that you have chosen and trained your model, it is time to deploy it to your\n",
    "clients system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def micro_service_classify_iris(datapoint):\n",
    "    \n",
    "  # make sure the provided datapoints adhere to the correct format for model input\n",
    "\n",
    "  # fetch your trained model\n",
    "  model = search.best_estimator_\n",
    "\n",
    "  # make prediction with the model\n",
    "  prediction = model.predict(datapoint)\n",
    "\n",
    "  return prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Analytics Cup, you need to export your prediction in a very specific output format. This is a csv file without an index and two columns, *id* and *prediction*. Note that the values in both columns need to be integer values, and especially in the *prediction* column either 1 or 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do: arrumar a celula abaixo com os nossos dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's assume that our id column is the index of the dataframe\n",
    "output = pd.DataFrame(df_flowers.variety)\n",
    "output['id'] = df_flowers.index\n",
    "output = output.rename(columns={'variety': 'prediction'})\n",
    "output = output.reindex(columns=[\"id\", \"prediction\"])\n",
    "output.to_csv('analzticscuppredictionfile.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
